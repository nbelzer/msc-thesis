{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
    "# import sys\n",
    "# sys.path.append('/root/of/repo/folder/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The workings of the Cooperative LRU Strategy\n",
    "\n",
    "In this small scale experiment we try to expose the effect of a different `node_trail_length`.  We compare it against a baseline LRU strategy. We will be experimenting with a `node_trail_length` of `1`, `2`, and `3` as well as an adapted version that uses all neighbouring nodes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Experiment Configuration"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_users = 1000\n",
    "no_iterations = 5000\n",
    "no_runs = 10\n",
    "trace_seeds = [ str(i) for i in range(no_runs) ]\n",
    "trail_lengths = [ 1, 2, 3 ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import make_dir, read_node_map\n",
    "\n",
    "resource_file = \"../dataset/out/dataset-resources-stats.csv\"\n",
    "pagemap_file = \"../dataset/out/page-map-clean.csv\"\n",
    "\n",
    "node_map_14 = read_node_map('./node_setups/14nodes.json') \n",
    "out_dir = make_dir('./out/experiment-cooplru/')\n",
    "\n",
    "lru_out_dir =  make_dir(f\"{out_dir}lru/\")\n",
    "cooplru_out_dir = make_dir(f\"{out_dir}/cooplru/\")\n",
    "neighbouring_lru_out_dir = make_dir(f\"{out_dir}/neighbouringlru/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traces\n",
    "\n",
    "For this experiment we will use both Zipf and Page-Map traces."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import load_or_generate_trace\n",
    "from simulation.generator.main_zipf import TraceConfig, Simulation\n",
    "from simulation.generator.main_page_map import UserTraceConfig, UserSimulation\n",
    "\n",
    "def generate_zipf_trace(seed: str, zipf_exponent: float):\n",
    "    trace_config = TraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations, zipf_exponent=zipf_exponent)\n",
    "    simulation = Simulation(trace_config, resource_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=simulation)\n",
    "\n",
    "def generate_page_map_trace(seed: str):\n",
    "    trace_config = UserTraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations)\n",
    "    user_simulation = UserSimulation(trace_config, pagemap_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=user_simulation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cooperative Strategy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import setup_nodes, setup_stats_file_writers, read_resource_map\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.strategy import CacheStrategy\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "from simulation.evaluator.strategy.neighbouring_lru import NeighbouringLRUStrategy\n",
    "from simulation.evaluator.strategy.cooperative_lru import CooperativeLRUStrategy\n",
    "from typing import Callable\n",
    "\n",
    "for node_trail in trail_lengths:\n",
    "    # To prevent two threads from trying to do the same, pre-make the dirs.\n",
    "    make_dir(f\"{cooplru_out_dir}/node_{node_trail}\")\n",
    "\n",
    "create_lru_setup = lambda nodes: (LRUStrategy(nodes), lru_out_dir)\n",
    "create_neighbouring_lru_setup = lambda nodes: (NeighbouringLRUStrategy(nodes, node_map={ node.identifier: node.neighbours for node in node_map_14.values() }), neighbouring_lru_out_dir)\n",
    "create_cooplru_setup = lambda node_trail_length: lambda nodes: (CooperativeLRUStrategy(nodes, node_trail_length=node_trail_length), f\"{cooplru_out_dir}/node_{node_trail_length}\")\n",
    "\n",
    "setups = [ create_lru_setup, create_neighbouring_lru_setup ] + [ create_cooplru_setup(trail_length) for trail_length in trail_lengths ]\n",
    "\n",
    "def run_strategy_experiment(trace, strategy_setup: Callable[[dict[str, dict[str, int]]], CacheStrategy], marker: str = \"\"):\n",
    "    nodes = setup_nodes(len(node_map_14), 1024 * 1024 * 1024)\n",
    "    strategy, strat_out_dir = strategy_setup(nodes)    \n",
    "    stats_writers = setup_stats_file_writers(nodes, strat_out_dir, marker=f\"n{len(nodes)}-{marker}\")\n",
    "    StrategyRunner(strategy, trace, read_resource_map(resource_file), stats_writers=stats_writers).perform()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the traces do not exist, pre-generate them and save them to file so they are available to each thread later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Pre-generating Traces\")\n",
    "for seed in trace_seeds:\n",
    "    trace_075 = generate_zipf_trace(seed=seed, zipf_exponent=0.75)\n",
    "    generate_page_map_trace(seed=seed)\n",
    "    trace_130 = generate_zipf_trace(seed=seed, zipf_exponent=1.30)\n",
    "print(\"All traces generated\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "load_075_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=0.75), '075')\n",
    "load_130_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=1.30), '130')\n",
    "load_page_map_trace = lambda seed: (generate_page_map_trace(seed=seed), 'page-map')\n",
    "\n",
    "trace_options = [ load_075_trace, load_130_trace, load_page_map_trace ]\n",
    "\n",
    "def run_experiment(trace_seed: str, trace_loader ):\n",
    "    trace, trace_marker = trace_loader(trace_seed)\n",
    "    print(trace_seed, trace_marker)\n",
    "    for setup in setups:\n",
    "        run_strategy_experiment(trace, setup, marker=f\"{trace_marker}-{trace_seed}\")\n",
    "    print(trace_seed, trace_marker, 'DONE')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the different experiments, you should set the pool size to the amount of experiments you can suppport on your machine.  In many cases this is limited by memory (instead of core count) due to the large size of the traces.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make use of multiprocess (over multiprocessing) if an \"AttributeError\" says it couldn't find `run_experiment`.\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    options = [ (seed, trace)\n",
    "                for seed in trace_seeds \n",
    "                for trace in trace_options ]\n",
    "    print(f\"Executing {len(options)} experiments...\")\n",
    "    with Pool(4) as p:\n",
    "        p.starmap(run_experiment, options, chunksize=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import load_runs_in_dir\n",
    "\n",
    "lru_runs = load_runs_in_dir(lru_out_dir)\n",
    "neighbouring_lru_runs = load_runs_in_dir(neighbouring_lru_out_dir)\n",
    "cooplru_runs = [ load_runs_in_dir(f\"{cooplru_out_dir}/node_{trail_length}\") for trail_length in trail_lengths ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import calc_ratio, calc_variance\n",
    "\n",
    "run_names = {\n",
    "    'ZipF-0.75': '-075-',\n",
    "    'Page-Map': '-page-map-',\n",
    "    'ZipF-1.30': '-130-'\n",
    "}\n",
    "\n",
    "def filter_runs_by(runs, match: str):\n",
    "    return [ r for r in runs if match in str(r[\"source\"]) ]\n",
    "\n",
    "def calc_over_setups(runs, strategy: str, calculation) -> list[float]:\n",
    "    filtered_runs = filter_runs_by(runs, strategy)\n",
    "    return [ calculation(run) for run in filtered_runs ]\n",
    "\n",
    "calc_average_hit_ratio = lambda run: calc_ratio(run['hits_total'][-1], run['misses_total'][-1])\n",
    "calc_average_byte_ratio = lambda run: calc_ratio(run['cache_bytes_total'][-1], run['origin_bytes_total'][-1])\n",
    "calc_average_neighbour_ratio = lambda run: calc_ratio(run['requests_to_neighbours_success'][-1], run['requests_to_neighbours'][-1])\n",
    "calc_average_neighbour_to_total = lambda run: run['requests_to_neighbours'][-1] / (run['hits_total'][-1] + run['misses_total'][-1])\n",
    "calc_average_neighbour_bytes = lambda run: run['neighbour_bytes_total'][-1] / (run['cache_bytes_total'][-1] + run['origin_bytes_total'][-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "table = defaultdict(list)\n",
    "\n",
    "def pretty_print_variance(variance: Tuple[float, float]) -> str:\n",
    "    return f\"{round(variance[0],3)}Â±{round(variance[1], 3)}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Average Hit Ratios"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calc_average_hit_ratio),\n",
    "        **{ f\"Co-Op N={trail_lengths[i]}\": calc_over_setups(cooplru_runs[i], generator_identifier, calc_average_hit_ratio)\n",
    "        for i in range(len(cooplru_runs)) },\n",
    "        \"Neighbours\": calc_over_setups(neighbouring_lru_runs, generator_identifier, calc_average_hit_ratio),\n",
    "    }\n",
    "    print(f\"{name} Average Hit Ratio\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Avarage Bandwidth Savings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "calculation = calc_average_byte_ratio\n",
    "\n",
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calculation),\n",
    "        **{f\"Co-Op N={trail_lengths[i]}\": calc_over_setups(cooplru_runs[i], generator_identifier, calculation)\n",
    "        for i in range(len(cooplru_runs))},\n",
    "        \"Neighbours\": calc_over_setups(neighbouring_lru_runs, generator_identifier, calculation),\n",
    "    }\n",
    "    print(f\"{name} Average Bandwidth Savings\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Average fraction of requests forwarded internally"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calc_average_neighbour_to_total),\n",
    "        **{f\"Co-Op N={trail_lengths[i]}\": calc_over_setups(cooplru_runs[i], generator_identifier, calc_average_neighbour_to_total)\n",
    "        for i in range(len(cooplru_runs))},\n",
    "        \"Neighbours\": calc_over_setups(neighbouring_lru_runs, generator_identifier, calc_average_neighbour_to_total),\n",
    "    }\n",
    "    print(f\"{name} Fraction of Internal Requests\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Average fraction of bandwidth used internally"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calc_average_neighbour_bytes),\n",
    "        **{f\"Co-Op N={trail_lengths[i]}\": calc_over_setups(cooplru_runs[i], generator_identifier, calc_average_neighbour_bytes)\n",
    "        for i in range(len(cooplru_runs))},\n",
    "        \"Neighbours\": calc_over_setups(neighbouring_lru_runs, generator_identifier, calc_average_neighbour_bytes),\n",
    "    }\n",
    "    print(f\"{name} Fraction of Internal Bandwidth\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "747677e8a21178563597d17909e53be1f2d30ac6bfe5206dd49919fb9b7c0458"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}