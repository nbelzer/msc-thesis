{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
    "# import sys\n",
    "# sys.path.append('/root/of/repo/folder/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The workings of the Profiles Strategy\n",
    "\n",
    "In this small scale experiment we try to expose the effect of a different `profile size`.  We compare it against a baseline LRU strategy. We will be experimenting with a `profile size` of `1`, `2`, and `3`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import make_dir, read_node_map\n",
    "\n",
    "resource_file = \"../dataset/out/dataset-resources-stats.csv\"\n",
    "pagemap_file = \"../dataset/out/page-map-clean.csv\"\n",
    "\n",
    "node_map_14 = read_node_map('./node_setups/14nodes.json') \n",
    "out_dir = make_dir('./out/experiment-profiles/')\n",
    "\n",
    "lru_out_dir =  make_dir(f\"{out_dir}lru/\")\n",
    "profiles_out_dir = make_dir(f\"{out_dir}/profiles/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_users = 1000\n",
    "no_iterations = 5000\n",
    "no_runs = 10\n",
    "trace_seeds = [ str(i) for i in range(no_runs) ]\n",
    "profile_sizes = [ 10, 100, 1000, 10000 ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traces"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import load_or_generate_trace\n",
    "from simulation.generator.main_zipf import TraceConfig, Simulation\n",
    "from simulation.generator.main_page_map import UserTraceConfig, UserSimulation\n",
    "\n",
    "def generate_zipf_trace(seed: str, zipf_exponent: float):\n",
    "    trace_config = TraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations, zipf_exponent=zipf_exponent)\n",
    "    simulation = Simulation(trace_config, resource_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=simulation)\n",
    "\n",
    "def generate_page_map_trace(seed: str):\n",
    "    trace_config = UserTraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations)\n",
    "    user_simulation = UserSimulation(trace_config, pagemap_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=user_simulation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Profile Strategy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import setup_nodes, setup_stats_file_writers, read_resource_map\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.strategy import CacheStrategy\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "from simulation.evaluator.strategy.profiles import ProfilesStrategy\n",
    "from typing import Callable\n",
    "\n",
    "for size in profile_sizes:\n",
    "    # To prevent two threads from trying to do the same, pre-make the dirs.\n",
    "    make_dir(f\"{profiles_out_dir}/size_{size}\")\n",
    "\n",
    "create_lru_setup = lambda nodes: (LRUStrategy(nodes), lru_out_dir)\n",
    "create_profiles_setup = lambda profile_size: lambda nodes: (ProfilesStrategy(nodes, profile_size=profile_size), f\"{profiles_out_dir}/size_{profile_size}\")\n",
    "\n",
    "setups = [ create_lru_setup ] + [ create_profiles_setup(size) for size in profile_sizes ]\n",
    "\n",
    "def run_strategy_experiment(trace, strategy_setup: Callable[[dict[str, dict[str, int]]], CacheStrategy], marker: str = \"\"):\n",
    "    nodes = setup_nodes(len(node_map_14), 1024 * 1024 * 1024)\n",
    "    strategy, strat_out_dir = strategy_setup(nodes)    \n",
    "    stats_writers = setup_stats_file_writers(nodes, strat_out_dir, marker=f\"n{len(nodes)}-{marker}\")\n",
    "    StrategyRunner(strategy, trace, read_resource_map(resource_file), stats_writers=stats_writers).perform()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Pre-generating Traces\")\n",
    "for seed in trace_seeds:\n",
    "    trace_075 = generate_zipf_trace(seed=seed, zipf_exponent=0.75)\n",
    "    generate_page_map_trace(seed=seed)\n",
    "    trace_130 = generate_zipf_trace(seed=seed, zipf_exponent=1.30)\n",
    "print(\"All traces generated\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "load_075_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=0.75), '075')\n",
    "load_130_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=1.30), '130')\n",
    "load_page_map_trace = lambda seed: (generate_page_map_trace(seed=seed), 'page-map')\n",
    "\n",
    "trace_options = [ load_075_trace, load_130_trace, load_page_map_trace ]\n",
    "\n",
    "def run_experiment(trace_seed: str, trace_loader, setup):\n",
    "    trace, trace_marker = trace_loader(trace_seed)\n",
    "    print(trace_seed, trace_marker)\n",
    "    run_strategy_experiment(trace, setup, marker=f\"{trace_marker}-{trace_seed}\")\n",
    "    print(trace_seed, trace_marker, 'DONE')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make use of multiprocess (over multiprocessing) if an \"AttributeError\" says it couldn't find `run_experiment`.\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    options = [ (seed, trace, setup)\n",
    "                for seed in trace_seeds \n",
    "                for trace in trace_options\n",
    "                for setup in setups ]\n",
    "    print(f\"Executing {len(options)} experiments...\")\n",
    "    with Pool(4) as p:\n",
    "        p.starmap(run_experiment, options, chunksize=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we take a quick detour and determine the average size for a single profile entry by taking the average length of our unique identifiers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import read_resource_map\n",
    "from sys import getsizeof\n",
    "\n",
    "resource_map = read_resource_map(resource_file)\n",
    "average_resource_uri_len = sum([ len(uri.encode('utf-8')) for uri in resource_map.keys() ]) / len(resource_map)\n",
    "print(f\"Average resource uri byte length: {average_resource_uri_len}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "l = []\n",
    "print(f\"Byte size of empty list: {getsizeof(l)}\")\n",
    "example_resource = list(resource_map.keys())[0]\n",
    "example_resource_2 = list(resource_map.keys())[1]\n",
    "l.append(example_resource)\n",
    "print(f\"Byte size of list with {len(example_resource.encode('utf-8'))}: {getsizeof(l)}\")\n",
    "l.append(example_resource_2)\n",
    "print(f\"Byte size of list with {len(example_resource_2.encode('utf-8'))}: {getsizeof(l)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import load_runs_in_dir\n",
    "\n",
    "lru_runs = simulation.evaluator(lru_out_dir)\n",
    "profiles_runs = [ simulation.evaluator(f\"{profiles_out_dir}/size_{size}/\") for size in profile_sizes ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import calc_ratio, calc_variance\n",
    "\n",
    "run_names = {\n",
    "    'ZipF-0.75': '-075-',\n",
    "    'Page-Map': '-page-map-',\n",
    "    'ZipF-1.30': '-130-'\n",
    "}\n",
    "\n",
    "def filter_runs_by(runs, match: str):\n",
    "    return [ r for r in runs if match in str(r[\"source\"]) ]\n",
    "\n",
    "def calc_over_setups(runs, strategy: str, calculation) -> list[float]:\n",
    "    filtered_runs = filter_runs_by(runs, strategy)\n",
    "    return [ calculation(run) for run in filtered_runs ]\n",
    "\n",
    "calc_average_hit_ratio = lambda run: calc_ratio(run['hits_total'][-1], run['misses_total'][-1])\n",
    "calc_average_byte_ratio = lambda run: calc_ratio(run['cache_bytes_total'][-1], run['origin_bytes_total'][-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple\n",
    "table = defaultdict(list)\n",
    "\n",
    "def pretty_print_variance(variance: Tuple[float, float]) -> str:\n",
    "    return f\"{round(variance[0],3)}Â±{round(variance[1], 3)}\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calc_average_hit_ratio),\n",
    "        **{ f\"Profiles Size={profile_sizes[i]}\": calc_over_setups(profiles_runs[i], generator_identifier, calc_average_hit_ratio)\n",
    "        for i in range(len(profiles_runs)) },\n",
    "    }\n",
    "    print(f\"{name} Average Hit Ratio\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "calculation = calc_average_byte_ratio\n",
    "\n",
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calculation),\n",
    "        **{f\"Profiles Size={profile_sizes[i]}\": calc_over_setups(profiles_runs[i], generator_identifier, calculation)\n",
    "        for i in range(len(profiles_runs))},\n",
    "    }\n",
    "    print(f\"{name} Average Bandwidth Savings\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "calc_average_neighbour_ratio = lambda run: calc_ratio(run['requests_to_neighbours_success'][-1], run['requests_to_neighbours'][-1])\n",
    "calc_average_neighbour_to_total = lambda run: run['requests_to_neighbours'][-1] / (run['hits_total'][-1] + run['misses_total'][-1])\n",
    "calc_average_neighbour_bytes = lambda run: run['neighbour_bytes_total'][-1] / (run['cache_bytes_total'][-1] + run['origin_bytes_total'][-1])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calc_average_neighbour_to_total),\n",
    "        **{f\"Profiles Size={profile_sizes[i]}\": calc_over_setups(profiles_runs[i], generator_identifier, calc_average_neighbour_to_total)\n",
    "        for i in range(len(profiles_runs))},\n",
    "    }\n",
    "    print(f\"{name} Fraction of Internal Requests\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for name, generator_identifier in run_names.items():\n",
    "    strategies = {\n",
    "        \"LRU\": calc_over_setups(lru_runs, generator_identifier, calc_average_neighbour_bytes),\n",
    "        **{f\"Profiles Size={profile_sizes[i]}\": calc_over_setups(profiles_runs[i], generator_identifier, calc_average_neighbour_bytes)\n",
    "        for i in range(len(profiles_runs))},\n",
    "    }\n",
    "    print(f\"{name} Fraction of Internal Bandwidth\")\n",
    "    for key, values in strategies.items():\n",
    "        print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "    print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "747677e8a21178563597d17909e53be1f2d30ac6bfe5206dd49919fb9b7c0458"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}