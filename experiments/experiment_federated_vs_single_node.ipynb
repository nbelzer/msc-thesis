{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
                "# import sys\n",
                "# sys.path.append('/root/of/repo/folder/')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from experiments.utils import generate_trace_if_not_exists, read_resource_map, setup_nodes, setup_stats_file_writers, make_dir, setup_node_map, TraceIteratorProxy, plot_with_error_bars, calc_variance, aggregate_runs_in_dir, calc_ratio_over\n",
                "from simulation.evaluator.strategy.runner import StrategyRunner\n",
                "from simulation.evaluator.strategy.federated import FederatedStrategy\n",
                "from simulation.evaluator.statistics.cache_metrics import CacheMetrics\n",
                "\n",
                "resource_file = \"../dataset/out/dataset-resources-stats.csv\"\n",
                "out_dir = make_dir('./out/experiment-federated-vs-single-node/')\n",
                "resource_map = read_resource_map(resource_file)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "# Federated versus Single Node\n",
                "\n",
                "In this experiment we evaluate whether a Federated Setup with 4 nodes of 1GB performs the same as 1 node with 4GB capacity.\n",
                "\n",
                "The difference between the two setups is the separate LRU strategies used in the 4 node setup.  While the distribution of files might be equal (due to the hash) this does not mean the file sizes are equally distributed over the four nodes.  This could generate a situation where we remove an item with age 2 from node A while there is an item with age 3 on node B.  This would not happen on the single node setup as there is only a single LRU list there.\n",
                "\n",
                "Therefore the main question of this experiment is whether this slight difference has a significant impact on the performance."
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Trace Generation\n",
                "We use a simple helper to load a trace from file, or generate it from a `TraceConfig` if it does not exist.  This saves time when we want to re-run the experiment as we do not need to invoke the generator again.  The trace is saved under a name based on the `trace_config` which means that a new trace will be generated automatically when you change the `TraceConfig` values.  As we save the trace using the `gzip` format it can take some time, after trace generation, to save the file.  The process is worth it however as it saves a significant amount of space.\n",
                "\n",
                "If you want to re-generate the trace on every run replace `generate_trace_if_not_exists` with `generate_trace` from `.utils`."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "from simulation.generator.main_zipf import TraceConfig, Simulation\n",
                "\n",
                "def generate_trace(seed: str):\n",
                "    trace_config = TraceConfig(node_map=setup_node_map(8), seed=seed, no_users=500, no_iterations=800)\n",
                "    simulation = Simulation(trace_config, resource_file)\n",
                "    return generate_trace_if_not_exists(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=simulation)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Multi Node Federated Setup\n",
                "In the multi-node setup we evaluate 4 nodes with 1024MB capacity working together according to the federated strategy."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def run_multi_node_experiment(trace, marker: str = \"\") -> dict[str, CacheMetrics]:\n",
                "    nodes = setup_nodes(8, 1024 * 1024 * 1024)\n",
                "    stats_writers = setup_stats_file_writers(nodes, make_dir(f\"{out_dir}multi-node\"), marker=marker)\n",
                "    strategy = FederatedStrategy(nodes)\n",
                "    return StrategyRunner(strategy, trace, resource_map, stats_writers=stats_writers).perform()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Single Node Federated Setup\n",
                "In the single-node setup we evaluate 1 node with 4096MB capacity.  \n",
                "To be able to use the same trace we use a `TraceIteratorProxy` that maps all requests to a single node."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def run_single_node_experiment(trace, marker: str = \"\") -> dict[str, CacheMetrics]:\n",
                "    trace_proxy = TraceIteratorProxy(trace.instructions, \n",
                "                                    proxy_map={ f\"cdn{i + 1}\": \"cdn1\" for i in range(8) })\n",
                "    nodes = setup_nodes(1, 8 * 1024 * 1024 * 1024)\n",
                "    stats_writers = setup_stats_file_writers(nodes, make_dir(f\"{out_dir}single-node/\"), marker=marker)\n",
                "    strategy = FederatedStrategy(nodes)\n",
                "    return StrategyRunner(strategy, trace_proxy, resource_map, stats_writers=stats_writers).perform()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "no_runs = 10\n",
                "trace_seeds = [ str(i) for i in range(no_runs) ]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Make use of multiprocess (over multiprocessing) if an \"AttributeError\" says it couldn't find `run_experiment`.\n",
                "from multiprocessing import Pool\n",
                "\n",
                "def run_experiment(trace_seed: str):\n",
                "    trace = generate_trace(seed=trace_seed)\n",
                "    run_single_node_experiment(trace, marker=trace_seed)\n",
                "    run_multi_node_experiment(trace, marker=trace_seed)\n",
                "\n",
                "if __name__ == '__main__':\n",
                "    print(\"Executing experiments...\")\n",
                "    with Pool(4) as p:\n",
                "        p.map(run_experiment, trace_seeds, chunksize=1)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Analyse\n",
                "We now need to analyse the results outputted to `multi_node_stats` and `single_node_stats`, potentially with some graphs."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "aggregated_multi_data = aggregate_runs_in_dir(f\"{out_dir}/multi-node\")\n",
                "aggregated_single_data = aggregate_runs_in_dir(f\"{out_dir}/single-node\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import matplotlib.pyplot as plt\n",
                "from palettable.colorbrewer.sequential import Greys_4\n",
                "from palettable.colorbrewer.diverging import PuOr_4\n",
                "greys = Greys_4.mpl_colors\n",
                "puor_4 = PuOr_4.mpl_colors\n",
                "import experiments.plotter.neat_plotter\n",
                "import numpy as np"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "multi_capacity = 1024 * 1024 * 1024\n",
                "single_capacity = 8 * multi_capacity\n",
                "\n",
                "multi_total_items = [ calc_variance([ y / multi_capacity for y in x ]) for x in aggregated_multi_data['cache_total']]\n",
                "single_total_items = [ calc_variance([ y / single_capacity for y in x ]) for x in aggregated_single_data['cache_total']]\n",
                "\n",
                "x_labels = range(len(single_total_items))\n",
                "plt.figure(num=None, figsize=(4, 4), dpi=300)\n",
                "\n",
                "plot_with_error_bars(plt, x_labels, single_total_items, label=\"Single Bytes Used\", color=puor_4[3])\n",
                "plot_with_error_bars(plt, x_labels, multi_total_items, label=\"Multi Bytes Used\", color=puor_4[2], linestyle='dashed')\n",
                "\n",
                "ylim = plt.ylim()\n",
                "plt.ylabel('Fraction of total capacity')\n",
                "plt.xlabel('Iteration')\n",
                "plt.legend(loc='lower right')\n",
                "plt.show()\n",
                "\n",
                "single_used, _ = zip(*single_total_items)\n",
                "multi_bytes, _ = zip(*multi_total_items)\n",
                "print(f\"Max Difference: {np.max(np.abs(np.array(single_used) - np.array(multi_bytes)))}\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "multi_average_hit_ratio = calc_ratio_over(aggregated_multi_data, 'hits_total', 'misses_total')\n",
                "single_average_hit_ratio = calc_ratio_over(aggregated_single_data, 'hits_total', 'misses_total')\n",
                "\n",
                "x_labels = range(len(single_average_hit_ratio))\n",
                "plt.figure(num=None, figsize=(4, 4), dpi=300)\n",
                "\n",
                "plot_with_error_bars(plt, x_labels, single_average_hit_ratio, label=\"Single Hit Ratio\", color=puor_4[1])\n",
                "plot_with_error_bars(plt, x_labels, multi_average_hit_ratio, label=\"Multi Hit Ratio\", color=puor_4[0], linestyle='dashed')\n",
                "\n",
                "plt.ylim(ylim)\n",
                "plt.ylabel('Hit Ratio')\n",
                "plt.xlabel('Iteration')\n",
                "plt.legend(loc='upper right')\n",
                "plt.show()\n",
                "\n",
                "single_hits, _ = zip(*single_average_hit_ratio)\n",
                "multi_hits, _ = zip(*multi_average_hit_ratio)\n",
                "print(f\"Max Difference: {np.max(np.abs(np.array(single_hits) - np.array(multi_hits)))}\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "multi_average_byte_ratio = calc_ratio_over(aggregated_multi_data, 'cache_bytes_total', 'origin_bytes_total')\n",
                "single_average_byte_ratio = calc_ratio_over(aggregated_single_data, 'cache_bytes_total', 'origin_bytes_total')\n",
                "\n",
                "x_labels = range(len(single_average_byte_ratio))\n",
                "plt.figure(num=None, figsize=(4, 4), dpi=300)\n",
                "\n",
                "plot_with_error_bars(plt, x_labels, single_average_byte_ratio, label=\"Single Byte Ratio\", color=puor_4[3])\n",
                "plot_with_error_bars(plt, x_labels, multi_average_byte_ratio, label=\"Multi Byte Ratio\", color=puor_4[2], linestyle='dashed')\n",
                "\n",
                "plt.ylim(ylim)\n",
                "plt.ylabel('Byte Ratio')\n",
                "plt.xlabel('Iteration')\n",
                "plt.legend(loc='upper right')\n",
                "plt.show()\n",
                "\n",
                "single_bytes, _ = zip(*single_average_byte_ratio)\n",
                "multi_bytes, _ = zip(*multi_average_byte_ratio)\n",
                "print(f\"Max Difference: {np.max(np.abs(np.array(single_bytes) - np.array(multi_bytes)))}\")"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.7",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.7 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "747677e8a21178563597d17909e53be1f2d30ac6bfe5206dd49919fb9b7c0458"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}