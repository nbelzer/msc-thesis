{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
    "# import sys\n",
    "# sys.path.append('/root/of/repo/folder/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Density Experiment\n",
    "\n",
    "The main reason for the introduction of different strategies is because of an issue, noted by previous works, where due to a large content churn and small population per edge node, reactive caching becomes unfeasible on smaller nodes. \n",
    "\n",
    "In this experiment we evaluate 4 different setups of edge nodes while keeping the number of users equal.  This means that in setups with more nodes the amount of users per node will decrease as will the number of requests.  We will use the LRU and Belady strategies to prove that this is indeed a problem.  To improve our confidence in the results we only plot the results after 10 runs with a confidence interval based on the standard deviation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import load_or_generate_trace, read_resource_map, setup_nodes, setup_stats_file_writers, make_dir, read_node_map, TraceIteratorProxy, calc_variance\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "\n",
    "resource_file = \"../dataset/out/dataset-resources-stats.csv\"\n",
    "pagemap_file = \"../dataset/out/page-map-clean.csv\"\n",
    "\n",
    "node_maps = { \n",
    "    1: read_node_map('./node_setups/1node.json'), \n",
    "    3: read_node_map('./node_setups/3nodes.json'), \n",
    "    8: read_node_map('./node_setups/8nodes.json'), \n",
    "    14: read_node_map('./node_setups/14nodes.json') \n",
    "}\n",
    "\n",
    "out_dir = make_dir('./out/experiment-density/')\n",
    "\n",
    "belady_out_dir = make_dir(f\"{out_dir}beladys/\")\n",
    "lru_out_dir =  make_dir(f\"{out_dir}lru/\")\n",
    "cooplru_out_dir = make_dir(f\"{out_dir}cooplru/\")\n",
    "profiles_out_dir = make_dir(f\"{out_dir}profiles/\")\n",
    "profiles_size100_out_dir = make_dir(f\"{out_dir}profiles-size100/\")\n",
    "federated_out_dir = make_dir(f\"{out_dir}federated/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_users = 1000\n",
    "no_iterations = 5000\n",
    "no_runs = 8\n",
    "trace_seeds = [ str(i) for i in range(no_runs) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traces\n",
    "We generate our traces using the largest `node_map`.  Then later on we use a `TraceIteratorProxy` to make sure that we map the right nodes to match the smaller edge setups.  The traces are generated using 5000 users over 1000 iterations for three different generators: _zipf of 0.75_, _zipf of 1.3_ and _page map_."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from simulation.generator.main_zipf import TraceConfig, Simulation\n",
    "from simulation.generator.main_page_map import UserTraceConfig, UserSimulation\n",
    "\n",
    "def generate_zipf_trace(seed: str, zipf_exponent: float):\n",
    "    trace_config = TraceConfig(node_map=node_maps[14], seed=seed, no_users=no_users, no_iterations=no_iterations, zipf_exponent=zipf_exponent)\n",
    "    simulation = Simulation(trace_config, resource_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=simulation)\n",
    "\n",
    "def generate_page_map_trace(seed: str):\n",
    "    trace_config = UserTraceConfig(node_map=node_maps[14], seed=seed, no_users=no_users, no_iterations=no_iterations)\n",
    "    user_simulation = UserSimulation(trace_config, pagemap_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=user_simulation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proxy Setups\n",
    "We set up three proxy maps to allow us to use the 1 node, 3 nodes, or 8 nodes setup with the 14 nodes traces."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proxy_map = {\n",
    "    1: { f\"cdn{i + 1}\": \"cdn1\" for i in range(14) },\n",
    "    3: { \"cdn1\": \"cdn1\", \"cdn2\": \"cdn2\", \"cdn3\": \"cdn3\", \"cdn4\": \"cdn2\", \"cdn5\": \"cdn2\", \"cdn6\": \"cdn1\", \"cdn7\": \"cdn3\", \"cdn8\": \"cdn3\", \"cdn9\": \"cdn1\", \"cdn10\": \"cdn2\", \"cdn11\": \"cdn1\", \"cdn12\": \"cdn3\", \"cdn13\": \"cdn3\", \"cdn14\": \"cdn3\" },\n",
    "    8: { \"cdn1\": \"cdn1\", \"cdn2\": \"cdn2\", \"cdn3\": \"cdn3\", \"cdn4\": \"cdn4\", \"cdn5\": \"cdn5\", \"cdn6\": \"cdn6\", \"cdn7\": \"cdn7\", \"cdn8\": \"cdn8\", \"cdn9\": \"cdn5\", \"cdn10\": \"cdn4\", \"cdn11\": \"cdn1\", \"cdn12\": \"cdn8\", \"cdn13\": \"cdn7\", \"cdn14\": \"cdn3\" },\n",
    "    14: { f\"cdn{i + 1}\": f\"cdn{i + 1}\" for i in range(14) },\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Strategies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from simulation.evaluator.strategy.belady_min import run_belady\n",
    "from .utils import read_resource_map\n",
    "\n",
    "def run_belady_experiment(trace, no_nodes: int, marker: str = \"\"):\n",
    "    trace_proxy = TraceIteratorProxy(trace.instructions, \n",
    "                                     proxy_map=proxy_map[no_nodes])\n",
    "    run_belady(trace_proxy, read_resource_map(resource_file), int(1024*1024*1024 / no_nodes), belady_out_dir, marker=f\"n{no_nodes}-{marker}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import setup_nodes, setup_stats_file_writers, read_resource_map\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.strategy import CacheStrategy\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "from simulation.evaluator.strategy.cooperative_lru import CooperativeLRUStrategy\n",
    "from simulation.evaluator.strategy.profiles import ProfilesStrategy\n",
    "from typing import Callable\n",
    "\n",
    "create_lru_setup = lambda nodes: (LRUStrategy(nodes), lru_out_dir)\n",
    "create_cooplru_setup = lambda nodes: (CooperativeLRUStrategy(nodes, node_trail_length=3), cooplru_out_dir)\n",
    "create_profiles_setup = lambda nodes: (ProfilesStrategy(nodes, ranking_timeout=5, profile_size=10000), profiles_out_dir)\n",
    "create_profiles_setup_100 = lambda nodes: (ProfilesStrategy(nodes, ranking_timeout=5, profile_size=100), profiles_size100_out_dir)\n",
    "\n",
    "setups = [ create_lru_setup, create_cooplru_setup, create_profiles_setup_100, create_profiles_setup ]\n",
    "\n",
    "def run_strategy_experiment(trace, strategy_setup: Callable[[dict[str, dict[str, int]]], CacheStrategy], no_nodes: int, marker: str = \"\"):\n",
    "    trace_proxy = TraceIteratorProxy(trace.instructions, \n",
    "                                     proxy_map=proxy_map[no_nodes])\n",
    "    nodes = setup_nodes(no_nodes, int(1024*1024*1024 / no_nodes))\n",
    "    strategy, strat_out_dir = strategy_setup(nodes)    \n",
    "    stats_writers = setup_stats_file_writers(nodes, strat_out_dir, marker=f\"n{len(nodes)}-{marker}\")\n",
    "    StrategyRunner(strategy, trace_proxy, read_resource_map(resource_file), stats_writers=stats_writers).perform()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Experiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pregenerate the traces and save them to file so that they are available for each thread."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Pre-generating Traces\")\n",
    "for seed in trace_seeds:\n",
    "    generate_zipf_trace(seed=seed, zipf_exponent=0.75)\n",
    "    generate_page_map_trace(seed=seed)\n",
    "    generate_zipf_trace(seed=seed, zipf_exponent=1.30)\n",
    "print(\"All traces generated\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "load_page_map_trace = lambda seed: (generate_page_map_trace(seed=seed), 'page-map')\n",
    "\n",
    "trace_options = [ load_page_map_trace ]\n",
    "\n",
    "def run_experiment(trace_seed: str, trace_loader, no_nodes: int):\n",
    "    trace, trace_marker = trace_loader(trace_seed)\n",
    "    print(trace_seed, trace_marker, no_nodes, 'STARTED')\n",
    "    run_belady_experiment(trace, no_nodes, marker=f\"{trace_marker}-{trace_seed}\")\n",
    "    for setup in setups:\n",
    "        run_strategy_experiment(trace, setup, no_nodes, marker=f\"{trace_marker}-{trace_seed}\")\n",
    "    print(trace_seed, trace_marker, no_nodes, 'DONE')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make use of multiprocess (over multiprocessing) if an \"AttributeError\" says it couldn't find `run_experiment`.\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    options = [ (seed, trace, no_nodes) \n",
    "                for seed in trace_seeds\n",
    "                for trace in trace_options \n",
    "                for no_nodes in node_maps.keys() ]\n",
    "    print(f\"Executing {len(options)} experiments...\")\n",
    "    with Pool(4) as p:\n",
    "        p.starmap(run_experiment, options, chunksize=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots\n",
    "To visualise the results we create two plots that show the main performance metrics: hit ratio and backhaul cost."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But first we have to load in all the data from the runs above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import load_runs_in_dir, calc_ratio\n",
    "import matplotlib.pyplot as plt\n",
    "from palettable.colorbrewer.diverging import PuOr_4\n",
    "puor_4 = PuOr_4.mpl_colors\n",
    "puor_4[3] = PuOr_4.mpl_colors[2]\n",
    "puor_4[2] = PuOr_4.mpl_colors[3]\n",
    "import experiments.plotter.neat_plotter\n",
    "\n",
    "belady_runs = load_runs_in_dir(belady_out_dir)\n",
    "lru_runs = load_runs_in_dir(lru_out_dir)\n",
    "cooplru_runs = load_runs_in_dir(cooplru_out_dir)\n",
    "profiles_runs = load_runs_in_dir(profiles_out_dir)\n",
    "profiles_runs_size100 = load_runs_in_dir(profiles_size100_out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def filter_runs_by(runs, match: str):\n",
    "    return [ r for r in runs if match in str(r[\"source\"]) ]\n",
    "\n",
    "def calc_over_setups(runs, strategy: str, calculation) -> list[float]:\n",
    "    setups = [ \"-n1-\", \"-n3-\", \"-n8-\", \"-n14-\" ]\n",
    "    datapoints = []\n",
    "    for setup in setups:\n",
    "        filtered_runs = filter_runs_by(filter_runs_by(runs, setup), strategy)\n",
    "        values = [ calculation(run) for run in filtered_runs ]\n",
    "        datapoints.append(calc_variance(values))\n",
    "    return datapoints\n",
    "\n",
    "calc_average_hit_ratio = lambda run: calc_ratio(run['hits_total'][-1], run['misses_total'][-1])\n",
    "calc_average_byte_ratio = lambda run: calc_ratio(run['cache_bytes_total'][-1], run['origin_bytes_total'][-1])\n",
    "calc_average_neighbour_to_total = lambda run: run['requests_to_neighbours'][-1] / (run['hits_total'][-1] + run['misses_total'][-1])\n",
    "calc_average_neighbour_bytes = lambda run: run['neighbour_bytes_total'][-1] / (run['cache_bytes_total'][-1] + run['origin_bytes_total'][-1])\n",
    "calc_average_neighbour_ratio = lambda run: calc_ratio(run['requests_to_neighbours_success'][-1], run['requests_to_neighbours'][-1])\n",
    "norm_neighbour_success = lambda run: calc_average_neighbour_ratio(run) * calc_average_neighbour_to_total(run)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Tuple\n",
    "from .utils import generate_comparison_plot\n",
    "\n",
    "from palettable.colorbrewer.qualitative import Dark2_8\n",
    "from palettable.colorbrewer.sequential import Greys_4\n",
    "dark_8 = Dark2_8.mpl_colors\n",
    "greys_4 = Greys_4.mpl_colors\n",
    "colors = [ greys_4[2] ] + dark_8[:3] + [ dark_8[2] ]\n",
    "\n",
    "def generate_plot(title: str, ylabel, strategies: dict[str, list[Tuple[float, float]]], colors, markers: list[str] = None, linestyles: list[str] = None):\n",
    "    if markers == None:\n",
    "        markers = ['.'] * len(strategies)\n",
    "    if linestyles == None:\n",
    "        linestyles = ['solid'] * len(strategies)\n",
    "    x_labels = [ round(no_users / no_nodes, 1) for no_nodes in node_maps.keys() ]\n",
    "    plt.figure(num=None, figsize=(3, 4), dpi=300)\n",
    "    generate_comparison_plot(plt, x_labels, strategies, colors=colors, markers=markers, linestyles=linestyles)\n",
    "    plt.ylim(bottom=0, top=1.0)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('Users per Node')\n",
    "    plt.title(title)\n",
    "    plt.xticks(x_labels, rotation=60)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generators = {\n",
    "    'Page-Map': '-page-map-',\n",
    "}\n",
    "\n",
    "calculations = {\n",
    "    'Average Hit Ratios': ('Hit Ratio', calc_average_hit_ratio),\n",
    "    'Average Bandwidth Savings': ('Bandwidth Savings', calc_average_byte_ratio),\n",
    "}\n",
    "\n",
    "for gen, generator in generators.items():\n",
    "    for calc, (ylabel, calculation) in calculations.items():\n",
    "        generate_plot(title=f\"{calc} {gen}\", ylabel=ylabel, strategies={\n",
    "            \"Belady's\": calc_over_setups(belady_runs, generator, calculation),\n",
    "            \"LRU\": calc_over_setups(lru_runs, generator, calculation),\n",
    "            \"Co-Op LRU\": calc_over_setups(cooplru_runs, generator, calculation),\n",
    "            \"Profiles (Size=10000)\": calc_over_setups(profiles_runs, generator, calculation),\n",
    "            \"Profiles (Size=100)\": calc_over_setups(profiles_runs_size100, generator, calculation),\n",
    "        }, colors=colors, markers=['.', 'v', 's', 'p', 'p', 'P'], linestyles=['dashed', 'solid', 'solid', 'solid', 'dashed', 'solid'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from palettable.colorbrewer.diverging import PuOr_4\n",
    "puor_4 = PuOr_4.mpl_colors\n",
    "\n",
    "generate_plot(title=\"Internal Requests Page Map\", ylabel='Internal Request Ratio', strategies={\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-page-map-', calc_average_neighbour_to_total),\n",
    "    \"Co-Op LRU Success\": calc_over_setups(cooplru_runs, '-page-map-', norm_neighbour_success),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-page-map-', calc_average_neighbour_to_total),\n",
    "    \"Profiles Success\": calc_over_setups(profiles_runs, '-page-map-', norm_neighbour_success),\n",
    "}, markers=['s', 's', 'p', 'p', 'P', 'P'], colors=[ dark_8[1], greys_4[1], dark_8[2], greys_4[2], dark_8[3] ], linestyles=['solid', 'dashed', 'solid', 'dashed', 'solid', 'dashed'] )"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "747677e8a21178563597d17909e53be1f2d30ac6bfe5206dd49919fb9b7c0458"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}