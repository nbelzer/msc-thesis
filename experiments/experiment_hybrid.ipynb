{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
    "# import sys\n",
    "# sys.path.append('/root/of/repo/folder/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Density Experiment\n",
    "\n",
    "The main reason for the introduction of different strategies is because of an issue, noted by previous works, where due to a large content churn and small population per edge node, reactive caching becomes unfeasible on smaller nodes. \n",
    "\n",
    "In this experiment we evaluate 4 different setups of edge nodes while keeping the number of users equal.  This means that in setups with more nodes the amount of users per node will decrease as will the number of requests.  We will use the LRU and Belady strategies to prove that this is indeed a problem.  To improve our confidence in the results we only plot the results after 10 runs with a confidence interval based on the standard deviation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import load_or_generate_trace, read_resource_map, setup_stats_file_writers, make_dir, read_node_map, TraceIteratorProxy, calc_variance\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "\n",
    "resource_file = \"../dataset/out/dataset-resources-stats.csv\"\n",
    "pagemap_file = \"../dataset/out/page-map-clean.csv\"\n",
    "\n",
    "node_maps = { \n",
    "    1: read_node_map('./node_setups/1node.json'), \n",
    "    3: read_node_map('./node_setups/3nodes.json'), \n",
    "    8: read_node_map('./node_setups/8nodes.json'), \n",
    "    14: read_node_map('./node_setups/14nodes.json') \n",
    "}\n",
    "\n",
    "out_dir = make_dir('./out/experiment-hybrid/')\n",
    "\n",
    "lru_out_dir =  make_dir(f\"{out_dir}lru/\")\n",
    "cooplru_out_dir = make_dir(f\"{out_dir}/cooplru/\")\n",
    "profiles_out_dir = make_dir(f\"{out_dir}/profiles/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_users = 1000\n",
    "no_iterations = 5000\n",
    "no_runs = 8\n",
    "trace_seeds = [ str(i) for i in range(no_runs) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traces\n",
    "We generate our traces using the largest `node_map`.  Then later on we use a `TraceIteratorProxy` to make sure that we map the right nodes to match the smaller edge setups.  The traces are generated using 5000 users over 1000 iterations for three different generators: _zipf of 0.75_, _zipf of 1.3_ and _page map_."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from simulation.generator.main_zipf import TraceConfig, Simulation\n",
    "from simulation.generator.main_page_map import UserTraceConfig, UserSimulation\n",
    "\n",
    "def generate_zipf_trace(seed: str, zipf_exponent: float):\n",
    "    trace_config = TraceConfig(node_map=node_maps[14], seed=seed, no_users=no_users, no_iterations=no_iterations, zipf_exponent=zipf_exponent)\n",
    "    simulation = Simulation(trace_config, resource_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=simulation)\n",
    "\n",
    "def generate_page_map_trace(seed: str):\n",
    "    trace_config = UserTraceConfig(node_map=node_maps[14], seed=seed, no_users=no_users, no_iterations=no_iterations)\n",
    "    user_simulation = UserSimulation(trace_config, pagemap_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=user_simulation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proxy Setups\n",
    "We set up a proxy map that matches the grouping of our nodes outlined in the Hybrid Experiment in the associated publication."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "proxy_map = { \"cdn1\": \"cdn1\", \"cdn2\": \"cdn1\", \"cdn3\": \"cdn3\", \"cdn4\": \"cdn1\", \"cdn5\": \"cdn2\", \"cdn6\": \"cdn1\", \"cdn7\": \"cdn3\", \"cdn8\": \"cdn3\", \"cdn9\": \"cdn2\", \"cdn10\": \"cdn1\", \"cdn11\": \"cdn2\", \"cdn12\": \"cdn2\", \"cdn13\": \"cdn3\", \"cdn14\": \"cdn3\" \n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Strategies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import setup_stats_file_writers, read_resource_map\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.strategy import CacheStrategy\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "from simulation.evaluator.strategy.cooperative_lru import CooperativeLRUStrategy\n",
    "from simulation.evaluator.strategy.profiles import ProfilesStrategy\n",
    "from typing import Callable\n",
    "\n",
    "create_lru_setup = lambda nodes: (LRUStrategy(nodes), lru_out_dir)\n",
    "create_cooplru_setup = lambda nodes: (CooperativeLRUStrategy(nodes, node_trail_length=3), cooplru_out_dir)\n",
    "create_profiles_setup = lambda nodes: (ProfilesStrategy(nodes, ranking_timeout=5, profile_size=10000), profiles_out_dir)\n",
    "\n",
    "setups = [ create_lru_setup, create_cooplru_setup, create_profiles_setup ]\n",
    "\n",
    "node_capacity = 1024*1024*1024\n",
    "node_setup = {\n",
    "    \"cdn1\": { \"capacity\": 5*node_capacity },\n",
    "    \"cdn2\": { \"capacity\": 4*node_capacity },\n",
    "    \"cdn3\": { \"capacity\": 5*node_capacity }\n",
    "}\n",
    "\n",
    "def run_strategy_experiment(trace, strategy_setup: Callable[[dict[str, dict[str, int]]], CacheStrategy], marker: str = \"\"):\n",
    "    trace_proxy = TraceIteratorProxy(trace.instructions, \n",
    "                                     proxy_map=proxy_map)\n",
    "    strategy, strat_out_dir = strategy_setup(node_setup)    \n",
    "    stats_writers = setup_stats_file_writers(node_setup, strat_out_dir, marker=f\"n{len(node_setup)}-{marker}\")\n",
    "    StrategyRunner(strategy, trace_proxy, read_resource_map(resource_file), stats_writers=stats_writers).perform()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Experiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Pre-generating Traces\")\n",
    "for seed in trace_seeds:\n",
    "    generate_zipf_trace(seed=seed, zipf_exponent=0.75)\n",
    "    generate_page_map_trace(seed=seed)\n",
    "    generate_zipf_trace(seed=seed, zipf_exponent=1.30)\n",
    "print(\"All traces generated\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "load_075_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=0.75), '075')\n",
    "load_130_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=1.30), '130')\n",
    "load_page_map_trace = lambda seed: (generate_page_map_trace(seed=seed), 'page-map')\n",
    "\n",
    "trace_options = [ load_075_trace, load_130_trace, load_page_map_trace ]\n",
    "\n",
    "def run_experiment(trace_seed: str, trace_loader):\n",
    "    trace, trace_marker = trace_loader(trace_seed)\n",
    "    print(trace_seed, trace_marker, 'STARTED')\n",
    "    for setup in setups:\n",
    "        run_strategy_experiment(trace, setup, marker=f\"{trace_marker}-{trace_seed}\")\n",
    "    print(trace_seed, trace_marker, 'DONE')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make use of multiprocess (over multiprocessing) if an \"AttributeError\" says it couldn't find `run_experiment`.\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    options = [ (seed, trace) \n",
    "                for seed in trace_seeds\n",
    "                for trace in trace_options ]\n",
    "    print(f\"Executing {len(options)} experiments...\")\n",
    "    with Pool(8) as p:\n",
    "        p.starmap(run_experiment, options, chunksize=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots\n",
    "To visualise the results we create two plots that show the main performance metrics: hit ratio and backhaul cost."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "But first we have to load in all the data from the runs above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from .utils import load_runs_in_dir, calc_ratio\n",
    "\n",
    "def filter_runs_by(runs, match: str):\n",
    "    return [ r for r in runs if match in str(r[\"source\"]) ]\n",
    "\n",
    "lru_runs = load_runs_in_dir(lru_out_dir)\n",
    "# Load non-hybrid runs with 128MiB for comparison, these are loaded from the storage experiment.\n",
    "cooplru_nh_runs = filter_runs_by(load_runs_in_dir(f\"./out/experiment-storage/cooplru/\"), '-134217728b-')\n",
    "profiles_nh_runs = filter_runs_by(load_runs_in_dir(f\"./out/experiment-storage/profiles/\"), '-134217728b-')\n",
    "# Load official runs.\n",
    "cooplru_runs = load_runs_in_dir(cooplru_out_dir)\n",
    "profiles_runs = load_runs_in_dir(profiles_out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calc_over_setups(runs, strategy: str, calculation) -> list[float]:\n",
    "    filtered_runs = filter_runs_by(runs, strategy)\n",
    "    return [ calculation(run) for run in filtered_runs ]\n",
    "\n",
    "calc_average_hit_ratio = lambda run: calc_ratio(run['hits_total'][-1], run['misses_total'][-1])\n",
    "calc_average_byte_ratio = lambda run: calc_ratio(run['cache_bytes_total'][-1], run['origin_bytes_total'][-1])\n",
    "calc_average_neighbour_to_total = lambda run: run['requests_to_neighbours'][-1] / (run['hits_total'][-1] + run['misses_total'][-1])\n",
    "calc_average_neighbour_bytes = lambda run: run['neighbour_bytes_total'][-1] / (run['cache_bytes_total'][-1] + run['origin_bytes_total'][-1])\n",
    "calc_average_neighbour_ratio = lambda run: calc_ratio(run['requests_to_neighbours_success'][-1], run['requests_to_neighbours'][-1])\n",
    "norm_neighbour_success = lambda run: calc_average_neighbour_ratio(run) * calc_average_neighbour_to_total(run)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def pretty_print_variance(variance: Tuple[float, float]) -> str:\n",
    "    return f\"{round(variance[0],3)}±{round(variance[1], 3)}\"\n",
    "\n",
    "generators = {\n",
    "    'ZipF-0.75': '-075-',\n",
    "    'Page-Map': '-page-map-',\n",
    "    'ZipF-1.30': '-130-'\n",
    "}\n",
    "\n",
    "calculations = {\n",
    "    'Average Hit Ratios': calc_average_hit_ratio,\n",
    "    # 'Internal Requests': calc_average_neighbour_to_total,\n",
    "    'Average Bandwidth Savings': calc_average_byte_ratio,\n",
    "    # 'Internal Bandwidth': calc_average_neighbour_bytes,\n",
    "}\n",
    "\n",
    "for gen, generator in generators.items():\n",
    "    for calc, calculation in calculations.items():\n",
    "        strategies={\n",
    "            \"Co-Op LRU NonHybrid\": calc_over_setups(cooplru_nh_runs, generator, calculation),\n",
    "            \"Profiles NonHybrid\": calc_over_setups(profiles_nh_runs, generator, calculation),\n",
    "            \"LRU\": calc_over_setups(lru_runs, generator, calculation),\n",
    "            \"Co-Op LRU\": calc_over_setups(cooplru_runs, generator, calculation),\n",
    "            \"Profiles\": calc_over_setups(profiles_runs, generator, calculation),\n",
    "        }\n",
    "        print(f\"{calc} {gen}\")\n",
    "        # for key, values in strategies.items():\n",
    "            # print(f\"\\t{key}:\\t{pretty_print_variance(calc_variance(values))}\")\n",
    "        print(\" & \".join([ pretty_print_variance(calc_variance(values)) for values in strategies.values() ]))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "747677e8a21178563597d17909e53be1f2d30ac6bfe5206dd49919fb9b7c0458"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}