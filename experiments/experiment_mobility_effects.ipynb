{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
    "# import sys\n",
    "# sys.path.append('/root/of/repo/folder/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Effects of mobility\n",
    "In this experiment we evaluate the relationship between the user mobility on the different generators and proposed strategies.  In different runs we evaluate whether our strategies perform better or worse when users move more frequently.  Here again we use the 14 node setup to allow for the most movement."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import make_dir, read_node_map\n",
    "\n",
    "resource_file = \"../dataset/out/dataset-resources-stats.csv\"\n",
    "pagemap_file = \"../dataset/out/page-map-clean.csv\"\n",
    "\n",
    "node_map_14 = read_node_map('./node_setups/14nodes.json') \n",
    "out_dir = make_dir('./out/experiment-mobility-effects/')\n",
    "\n",
    "belady_out_dir = make_dir(f\"{out_dir}beladys/\")\n",
    "lru_out_dir =  make_dir(f\"{out_dir}lru/\")\n",
    "cooplru_out_dir = make_dir(f\"{out_dir}/cooplru/\")\n",
    "profiles_out_dir = make_dir(f\"{out_dir}/profiles/\")\n",
    "federated_out_dir = make_dir(f\"{out_dir}/federated/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment Configuration\n",
    "We test 4 different mobility values, `once-every-1000`, `once-every-100`, `once-every-10`, `every-2` iterations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_users = 1000\n",
    "no_iterations = 5000\n",
    "no_runs = 10\n",
    "trace_seeds = [ str(i) for i in range(no_runs) ]\n",
    "cache_capacity = 1024 * 1024 * 1024\n",
    "\n",
    "mobility_speeds = [ 0.001, 0.01, 0.1, 0.5 ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traces\n",
    "We generate traces for the largest node setup as mentioned above."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import load_or_generate_trace\n",
    "from simulation.generator.main_zipf import TraceConfig, Simulation\n",
    "from simulation.generator.main_page_map import UserTraceConfig, UserSimulation\n",
    "\n",
    "def generate_zipf_trace(seed: str, zipf_exponent: float, user_mobility: float = 0.05):\n",
    "    trace_config = TraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations, zipf_exponent=zipf_exponent, move_chance=user_mobility)\n",
    "    simulation = Simulation(trace_config, resource_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=simulation)\n",
    "\n",
    "def generate_page_map_trace(seed: str, user_mobility: float = 0.05):\n",
    "    trace_config = UserTraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations, move_chance=user_mobility)\n",
    "    user_simulation = UserSimulation(trace_config, pagemap_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=user_simulation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Strategies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from simulation.evaluator.strategy.belady_min import run_belady\n",
    "from experiments.utils import read_resource_map\n",
    "\n",
    "def run_belady_experiment(trace, marker: str = \"\"):\n",
    "    run_belady(trace, read_resource_map(resource_file), cache_capacity, belady_out_dir, marker=f\"n{len(node_map_14)}-{marker}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import setup_nodes, setup_stats_file_writers, read_resource_map\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.strategy import CacheStrategy\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "from simulation.evaluator.strategy.cooperative_lru import CooperativeLRUStrategy\n",
    "from simulation.evaluator.strategy.profiles import ProfilesStrategy\n",
    "from simulation.evaluator.strategy.federated import FederatedStrategy\n",
    "from typing import Callable\n",
    "\n",
    "create_lru_setup = lambda nodes: (LRUStrategy(nodes), lru_out_dir)\n",
    "create_cooplru_setup = lambda nodes: (CooperativeLRUStrategy(nodes, node_trail_length=3), cooplru_out_dir)\n",
    "create_profiles_setup = lambda nodes: (ProfilesStrategy(nodes, ranking_timeout=5), profiles_out_dir)\n",
    "create_federated_setup = lambda nodes: (FederatedStrategy(nodes), federated_out_dir)\n",
    "\n",
    "setups = [ create_lru_setup, create_cooplru_setup, create_profiles_setup, create_federated_setup ]\n",
    "\n",
    "def run_strategy_experiment(trace, strategy_setup: Callable[[dict[str, dict[str, int]]], CacheStrategy], marker: str = \"\"):\n",
    "    nodes = setup_nodes(len(node_map_14), cache_capacity)\n",
    "    strategy, strat_out_dir = strategy_setup(nodes)    \n",
    "    stats_writers = setup_stats_file_writers(nodes, strat_out_dir, marker=f\"n{len(nodes)}-{marker}\")\n",
    "    StrategyRunner(strategy, trace, read_resource_map(resource_file), stats_writers=stats_writers).perform()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experiment\n",
    "In contrast to previous experiments we need to generate a new trace for every setup as the mobility speed is part of the trace.  This means there is no need to pre-generate the traces, instead, each process can generate their own based on the experiment setup.  We only use the Page Map traces as due to the specialisation of users popularity moves around."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "load_page_map_trace = lambda seed, speed: (generate_page_map_trace(seed=seed, user_mobility=speed), 'page-map')\n",
    "\n",
    "trace_options = [ load_page_map_trace ]\n",
    "\n",
    "def run_experiment(trace_seed: str, trace_loader, mobility_speed: float):\n",
    "    trace, trace_marker = trace_loader(trace_seed, mobility_speed)\n",
    "    print(trace_seed, trace_marker, mobility_speed)\n",
    "    run_belady_experiment(trace, marker=f\"{trace_marker}-speed-{mobility_speed}-{trace_seed}\")\n",
    "    for setup in setups:\n",
    "        run_strategy_experiment(trace, setup, marker=f\"{trace_marker}-speed-{mobility_speed}-{trace_seed}\")\n",
    "    print(trace_seed, trace_marker, mobility_speed, 'DONE')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make use of multiprocess (over multiprocessing) if an \"AttributeError\" says it couldn't find `run_experiment`.\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    options = [ (seed, trace, speed)\n",
    "                for seed in trace_seeds \n",
    "                for trace in trace_options \n",
    "                for speed in mobility_speeds ]\n",
    "    print(f\"Executing {len(options)} experiments...\")\n",
    "    with Pool(4) as p:\n",
    "        p.starmap(run_experiment, options, chunksize=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import load_runs_in_dir\n",
    "from palettable.colorbrewer.qualitative import Dark2_8\n",
    "from palettable.colorbrewer.sequential import Greys_4\n",
    "dark_8 = Dark2_8.mpl_colors\n",
    "greys_4 = Greys_4.mpl_colors\n",
    "import experiments.plotter.neat_plotter\n",
    "\n",
    "def load_complete_runs(in_dir):\n",
    "    return [ r for r in load_runs_in_dir(in_dir) if r['iteration'][-1] == 999]\n",
    "\n",
    "belady_runs = load_runs_in_dir(belady_out_dir)\n",
    "lru_runs = load_runs_in_dir(lru_out_dir)\n",
    "cooplru_runs = load_runs_in_dir(cooplru_out_dir)\n",
    "profiles_runs = load_complete_runs(profiles_out_dir)\n",
    "federated_runs = load_runs_in_dir(federated_out_dir)\n",
    "print(len(profiles_runs), len(federated_runs))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import calc_ratio, calc_variance\n",
    "\n",
    "def filter_runs_by(runs, match: str):\n",
    "    return [ r for r in runs if match in str(r[\"source\"]) ]\n",
    "\n",
    "def calc_over_setups(runs, strategy: str, calculation) -> list[float]:\n",
    "    setups = [ f\"-speed-{s}-\" for s in mobility_speeds ]\n",
    "    datapoints = []\n",
    "    for setup in setups:\n",
    "        filtered_runs = filter_runs_by(filter_runs_by(runs, setup), strategy)\n",
    "        values = [ calculation(run) for run in filtered_runs ]\n",
    "        datapoints.append(calc_variance(values))\n",
    "    return datapoints\n",
    "\n",
    "calc_average_hit_ratio = lambda run: calc_ratio(run['hits_total'][-1], run['misses_total'][-1])\n",
    "calc_average_byte_ratio = lambda run: calc_ratio(run['cache_bytes_total'][-1], run['origin_bytes_total'][-1])\n",
    "calc_average_neighbour_ratio = lambda run: calc_ratio(run['requests_to_neighbours_success'][-1], run['requests_to_neighbours'][-1])\n",
    "calc_average_neighbour_to_total = lambda run: run['requests_to_neighbours'][-1] / (run['hits_total'][-1] + run['misses_total'][-1])\n",
    "calc_average_neighbour_bytes = lambda run: run['neighbour_bytes_total'][-1] / (run['cache_bytes_total'][-1] + run['origin_bytes_total'][-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from experiments.utils import generate_comparison_plot\n",
    "from typing import Tuple\n",
    "colors = [ greys_4[2] ] + dark_8\n",
    "\n",
    "def generate_plot(title: str, ylabel, strategies: dict[str, list[Tuple[float, float]]], ylim=(0, 1.0), markers=['.', 'v', 's', 'p', 'P', '*', 'D'], colors=colors, linestyles=['dashed', 'solid', 'solid', 'solid', 'solid']):\n",
    "    x_labels = [ 1 / speed for speed in mobility_speeds ]\n",
    "    plt.figure(num=None, figsize=(3, 4), dpi=300)\n",
    "    generate_comparison_plot(plt, x_labels, strategies, colors=colors, linestyles=linestyles, markers=markers)\n",
    "    plt.xscale('log')\n",
    "    plt.ylim(ylim)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('Iterations between user movement')\n",
    "    plt.title(title)\n",
    "    plt.xticks(x_labels, rotation=60)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot(title=\"Average Hit Ratios Page Map\", ylabel='Hit Ratio', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-page-map-', calc_average_hit_ratio)\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot(title=\"Average Bandwidth Saved Page-Map\", ylabel='Fraction of Bandwidth Saved', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-page-map-', calc_average_byte_ratio)\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Internal Bandwidth"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from palettable.colorbrewer.diverging import PuOr_4\n",
    "puor_4 = PuOr_4.mpl_colors\n",
    "\n",
    "norm_neighbour_success = lambda run: calc_average_neighbour_ratio(run) * calc_average_neighbour_to_total(run)\n",
    "\n",
    "generate_plot(title=\"Internal Requests Page Map\", ylabel='Internal Request Ratio', strategies={\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-page-map-', calc_average_neighbour_to_total),\n",
    "    \"Co-Op LRU Success\": calc_over_setups(cooplru_runs, '-page-map-', norm_neighbour_success),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-page-map-', calc_average_neighbour_to_total),\n",
    "    \"Profiles Success\": calc_over_setups(profiles_runs, '-page-map-', norm_neighbour_success),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-page-map-', calc_average_neighbour_to_total),\n",
    "}, markers=['s', 's', 'p', 'p', 'P', 'P'], colors=[ dark_8[1], greys_4[1], dark_8[2], greys_4[2], dark_8[3] ], linestyles=['solid', 'dashed', 'solid', 'dashed', 'solid', 'dashed'] )\n",
    "print(sum([ avg for avg, std in calc_over_setups(federated_runs, '-page-map-', calc_average_neighbour_to_total) ]) / 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot(title=\"Internal Bandwidth Page Map\", ylabel='Normalised Average Internal Bandwidth Used', strategies={\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-page-map-', calc_average_neighbour_bytes),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-page-map-', calc_average_neighbour_bytes),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-page-map-', calc_average_neighbour_bytes)\n",
    "}, markers=['s', 'p', 'P'], colors=dark_8[1:], linestyles=['solid'] * 3)\n",
    "print(sum([ avg for avg, std in calc_over_setups(federated_runs, '-page-map-', calc_average_neighbour_bytes) ]) / 4)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "747677e8a21178563597d17909e53be1f2d30ac6bfe5206dd49919fb9b7c0458"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}