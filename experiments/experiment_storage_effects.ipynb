{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
    "# import sys\n",
    "# sys.path.append('/root/of/repo/folder/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The relationship with storage capacity\n",
    "\n",
    "In this experiment we evaluate the relationship between storage and performance for the different generators and strategies.  We perform a total of three runs for each setup to improve our confidence in the results.\n",
    "\n",
    "The setup will use the worst performing edge node setup of 14 nodes to see what kind of improvements can be found by increasing the storage capacity.  In addition to the different strategies we also plot belady's min performance as a comparison."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import make_dir, read_node_map\n",
    "\n",
    "resource_file = \"../dataset/out/dataset-resources-stats.csv\"\n",
    "pagemap_file = \"../dataset/out/page-map-clean.csv\"\n",
    "\n",
    "node_map_14 = read_node_map('./node_setups/14nodes.json') \n",
    "out_dir = make_dir('./out/experiment-storage/')\n",
    "\n",
    "belady_out_dir = make_dir(f\"{out_dir}beladys/\")\n",
    "lru_out_dir =  make_dir(f\"{out_dir}lru/\")\n",
    "cooplru_out_dir = make_dir(f\"{out_dir}/cooplru/\")\n",
    "profiles_out_dir = make_dir(f\"{out_dir}/profiles/\")\n",
    "federated_out_dir = make_dir(f\"{out_dir}/federated/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some general configuration on the experiment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_users = 1000\n",
    "no_iterations = 5000\n",
    "no_runs = 10\n",
    "trace_seeds = [ str(i) for i in range(no_runs) ]\n",
    "\n",
    "def mib_to_bytes(mib: int):\n",
    "    return mib * 1024 * 1024\n",
    "\n",
    "capacities = [ mib_to_bytes(64), mib_to_bytes(128), mib_to_bytes(256), mib_to_bytes(512), mib_to_bytes(1024), mib_to_bytes(2048) ]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Traces\n",
    "In this experiment we want to look at the performance differences gained by increasing the storage capacity of our nodes to aid with the limited scope effect.  We use the worst performing setup with 14 edge nodes and evaluate the setup with different storage capacities per node."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import load_or_generate_trace\n",
    "from simulation.generator.main_zipf import TraceConfig, Simulation\n",
    "from simulation.generator.main_page_map import UserTraceConfig, UserSimulation\n",
    "\n",
    "def generate_zipf_trace(seed: str, zipf_exponent: float):\n",
    "    trace_config = TraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations, zipf_exponent=zipf_exponent)\n",
    "    simulation = Simulation(trace_config, resource_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=simulation)\n",
    "\n",
    "def generate_page_map_trace(seed: str):\n",
    "    trace_config = UserTraceConfig(node_map=node_map_14, seed=seed, no_users=no_users, no_iterations=no_iterations)\n",
    "    user_simulation = UserSimulation(trace_config, pagemap_file)\n",
    "    return load_or_generate_trace(f\"{out_dir}/{trace_config.to_filename()}.trace.gz\", simulation=user_simulation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Strategies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Belady's MIN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from simulation.evaluator.strategy.belady_min import run_belady\n",
    "from experiments.utils import read_resource_map\n",
    "\n",
    "def run_belady_experiment(trace, cache_capacity: int, marker: str = \"\"):\n",
    "    run_belady(trace, read_resource_map(resource_file), cache_capacity, belady_out_dir, marker=f\"n{len(node_map_14)}-{marker}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Strategies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import setup_nodes, setup_stats_file_writers, read_resource_map\n",
    "from simulation.evaluator.strategy.runner import StrategyRunner\n",
    "from simulation.evaluator.strategy.strategy import CacheStrategy\n",
    "from simulation.evaluator.strategy.lru import LRUStrategy\n",
    "from simulation.evaluator.strategy.cooperative_lru import CooperativeLRUStrategy\n",
    "from simulation.evaluator.strategy.profiles import ProfilesStrategy\n",
    "from simulation.evaluator.strategy.federated import FederatedStrategy\n",
    "from typing import Callable\n",
    "\n",
    "create_lru_setup = lambda nodes: (LRUStrategy(nodes), lru_out_dir)\n",
    "create_cooplru_setup = lambda nodes: (CooperativeLRUStrategy(nodes, node_trail_length=3), cooplru_out_dir)\n",
    "create_profiles_setup = lambda nodes: (ProfilesStrategy(nodes, ranking_timeout=5, profile_size=10000), profiles_out_dir)\n",
    "create_federated_setup = lambda nodes: (FederatedStrategy(nodes), federated_out_dir)\n",
    "\n",
    "setups = [ create_lru_setup, create_cooplru_setup, create_profiles_setup, create_federated_setup ]\n",
    "\n",
    "def run_strategy_experiment(trace, strategy_setup: Callable[[dict[str, dict[str, int]]], CacheStrategy], cache_capacity: int, marker: str = \"\"):\n",
    "    nodes = setup_nodes(len(node_map_14), cache_capacity)\n",
    "    strategy, strat_out_dir = strategy_setup(nodes)    \n",
    "    stats_writers = setup_stats_file_writers(nodes, strat_out_dir, marker=f\"n{len(nodes)}-{marker}\")\n",
    "    StrategyRunner(strategy, trace, read_resource_map(resource_file), stats_writers=stats_writers).perform()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Experiment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Pre-generating Traces\")\n",
    "for seed in trace_seeds:\n",
    "    trace_075 = generate_zipf_trace(seed=seed, zipf_exponent=0.75)\n",
    "    generate_page_map_trace(seed=seed)\n",
    "    trace_130 = generate_zipf_trace(seed=seed, zipf_exponent=1.30)\n",
    "print(\"All traces generated\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "load_075_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=0.75), '075')\n",
    "load_130_trace = lambda seed: (generate_zipf_trace(seed=seed, zipf_exponent=1.30), '130')\n",
    "load_page_map_trace = lambda seed: (generate_page_map_trace(seed=seed), 'page-map')\n",
    "\n",
    "trace_options = [ load_075_trace, load_130_trace, load_page_map_trace ]\n",
    "\n",
    "def run_experiment(trace_seed: str, trace_loader, capacity: int):\n",
    "    trace, trace_marker = trace_loader(trace_seed)\n",
    "    print(trace_seed, trace_marker, capacity)\n",
    "    run_belady_experiment(trace, capacity, marker=f\"{trace_marker}-{capacity}b-{trace_seed}\")\n",
    "    for setup in setups:\n",
    "        run_strategy_experiment(trace, setup, capacity, marker=f\"{trace_marker}-{capacity}b-{trace_seed}\")\n",
    "    print(trace_seed, trace_marker, capacity, 'DONE')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make use of multiprocess (over multiprocessing) if an \"AttributeError\" says it couldn't find `run_experiment`.\n",
    "from multiprocessing import Pool\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    options = [ (seed, trace, capacity)\n",
    "                for seed in trace_seeds \n",
    "                for trace in trace_options \n",
    "                for capacity in capacities ]\n",
    "    print(f\"Executing {len(options)} experiments...\")\n",
    "    with Pool(8) as p:\n",
    "        p.starmap(run_experiment, options, chunksize=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plots\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import load_runs_in_dir\n",
    "from palettable.colorbrewer.qualitative import Dark2_8\n",
    "from palettable.colorbrewer.sequential import Greys_4\n",
    "dark_8 = Dark2_8.mpl_colors\n",
    "greys_4 = Greys_4.mpl_colors\n",
    "import experiments.plotter.neat_plotter\n",
    "\n",
    "belady_runs = load_runs_in_dir(belady_out_dir)\n",
    "lru_runs = load_runs_in_dir(lru_out_dir)\n",
    "cooplru_runs = load_runs_in_dir(cooplru_out_dir)\n",
    "profiles_runs = load_runs_in_dir(profiles_out_dir)\n",
    "federated_runs = load_runs_in_dir(federated_out_dir)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from experiments.utils import calc_ratio, calc_variance\n",
    "\n",
    "def filter_runs_by(runs, match: str):\n",
    "    return [ r for r in runs if match in str(r[\"source\"]) ]\n",
    "\n",
    "def calc_over_setups(runs, strategy: str, calculation) -> list[float]:\n",
    "    setups = [ f\"-{c}b-\" for c in capacities ]\n",
    "    datapoints = []\n",
    "    for setup in setups:\n",
    "        filtered_runs = filter_runs_by(filter_runs_by(runs, setup), strategy)\n",
    "        values = [ calculation(run) for run in filtered_runs ]\n",
    "        datapoints.append(calc_variance(values))\n",
    "    return datapoints\n",
    "\n",
    "calc_average_hit_ratio = lambda run: calc_ratio(run['hits_total'][-1], run['misses_total'][-1])\n",
    "calc_average_byte_ratio = lambda run: calc_ratio(run['cache_bytes_total'][-1], run['origin_bytes_total'][-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "calc_average_capacity_used = lambda run: run['cache_total'][-1] / 1024 / 1024\n",
    "\n",
    "datapoints = []\n",
    "for setup in [ '-075-', '-page-map-', '-130-' ]:\n",
    "    filtered_runs = filter_runs_by(filter_runs_by(belady_runs, setup), '-4294967296b-')\n",
    "    datapoints.append(calc_variance([ calc_average_capacity_used(run) for run in filtered_runs ]))\n",
    "print(datapoints)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from experiments.utils import generate_comparison_plot\n",
    "from typing import Tuple\n",
    "from palettable.colorbrewer.qualitative import Dark2_8\n",
    "from palettable.colorbrewer.sequential import Greys_4\n",
    "dark_8 = Dark2_8.mpl_colors\n",
    "greys_4 = Greys_4.mpl_colors\n",
    "colors = [ greys_4[2] ] + dark_8\n",
    "\n",
    "def generate_plot(title: str, ylabel, strategies: dict[str, list[Tuple[float, float]]], ylim=(0, 1.0)):\n",
    "    x_labels = [ c for c in capacities ]\n",
    "    plt.figure(num=None, figsize=(3, 4), dpi=300)\n",
    "    generate_comparison_plot(plt, x_labels, strategies, colors=colors, markers=['.', 'v', 's', 'p', 'P', '*', 'D'], linestyles=['dashed', 'solid', 'solid', 'solid', 'solid'])\n",
    "    plt.ylim(ylim)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('Cache Capacity (MiB, log base-2)')\n",
    "    plt.title(title)\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.xticks(x_labels, labels=['64', '128', '256', '512', '1024', '2048'], rotation=90)\n",
    "    plt.tick_params(labelsize='10')\n",
    "    plt.axis()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def generate_plot_zipf130(title: str, ylabel, strategies: dict[str, list[Tuple[float, float]]], ylim=(0, 1.0)):\n",
    "    x_labels = [ c for c in capacities ][:-1]\n",
    "    plt.figure(num=None, figsize=(3, 4), dpi=300)\n",
    "    generate_comparison_plot(plt, x_labels, strategies, colors=colors, markers=['.', 'v', 's', 'p', 'P', '*', 'D'], linestyles=['dashed', 'solid', 'solid', 'solid', 'solid'])\n",
    "    plt.ylim(ylim)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel('Cache Capacity (MiB, log base-2)')\n",
    "    plt.title(title)\n",
    "    plt.xscale('log', base=2)\n",
    "    plt.xticks(x_labels, labels=['64', '128', '256', '512', '1024', '2048'][:-1], rotation=90)\n",
    "    plt.tick_params(labelsize='10')\n",
    "    plt.axis()\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot(title=\"Average Hit Ratios Zipf-0.75\", ylabel='Hit Ratio', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-075-', calc_average_hit_ratio),\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-075-', calc_average_hit_ratio),\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-075-', calc_average_hit_ratio),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-075-', calc_average_hit_ratio),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-075-', calc_average_hit_ratio)\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot(title=\"Average Hit Ratios Page Map\", ylabel='Hit Ratio', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-page-map-', calc_average_hit_ratio),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-page-map-', calc_average_hit_ratio)\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot_zipf130(title=\"Average Hit Ratios Zipf-1.30\", ylabel='Hit Ratio', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-130-', calc_average_hit_ratio)[:-1],\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-130-', calc_average_hit_ratio)[:-1],\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-130-', calc_average_hit_ratio)[:-1],\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-130-', calc_average_hit_ratio)[:-1],\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-130-', calc_average_hit_ratio)[:-1]\n",
    "}, ylim=(0.5, 1.0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot(title=\"Average Bandwidth Saved Zipf-0.75\", ylabel='Fraction of Bandwidth Saved', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-075-', calc_average_byte_ratio),\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-075-', calc_average_byte_ratio),\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-075-', calc_average_byte_ratio),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-075-', calc_average_byte_ratio),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-075-', calc_average_byte_ratio)\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot(title=\"Average Bandwidth Saved Page-Map\", ylabel='Fraction of Bandwidth Saved', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-page-map-', calc_average_byte_ratio),\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-page-map-', calc_average_byte_ratio)\n",
    "})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generate_plot_zipf130(title=\"Average Bandwidth Saved Zipf-1.30\", ylabel='Fraction of Bandwidth Saved', strategies={\n",
    "    \"Belady's\": calc_over_setups(belady_runs, '-130-', calc_average_byte_ratio)[:-1],\n",
    "    \"LRU\": calc_over_setups(lru_runs, '-130-', calc_average_byte_ratio)[:-1],\n",
    "    \"Co-Op LRU\": calc_over_setups(cooplru_runs, '-130-', calc_average_byte_ratio)[:-1],\n",
    "    \"Profiles\": calc_over_setups(profiles_runs, '-130-', calc_average_byte_ratio)[:-1],\n",
    "    \"Federated\": calc_over_setups(federated_runs, '-130-', calc_average_byte_ratio)[:-1]\n",
    "}, ylim=(0.5, 1.0))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "747677e8a21178563597d17909e53be1f2d30ac6bfe5206dd49919fb9b7c0458"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}