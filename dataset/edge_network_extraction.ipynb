{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Personally I had to add the root folder of the repo to the sys.path.  If certain imports do not work you should uncomment and set the following.\n",
    "# import sys\n",
    "# sys.path.append('/root/of/repo/folder/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Edge Node graph from OpenStreetMaps\n",
    "\n",
    "To create a map of edge nodes that somewhat resembles population density and shows clusters around dense areas we want to use the national train connections.  We make use of the [OpenStreetMap Data](https://download.geofabrik.de) and use the [osmium-tool](https://osmcode.org/osmium-tool/manual.html) to query the data.  To interface with the file from Python we use [PyOsmium](https://docs.osmcode.org/pyosmium/latest/).\n",
    "\n",
    "We use the `StationHandler` to extract the `nodes` and `ways` in the dataset that represent `stop_position`s for trains and `railway`s, respectively."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import osmium\n",
    "\n",
    "osm_file = \"./out/netherlands-latest.osm.bz2\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class StationHandler(osmium.SimpleHandler):\n",
    "    def __init__(self):\n",
    "        osmium.SimpleHandler.__init__(self)\n",
    "        self.stops = {}\n",
    "        self.stations = {}\n",
    "        self.track = []\n",
    "    \n",
    "    def node(self, n):\n",
    "        # We filter on n\\train=yes to get only train related nodes, additionally we found that stations are the only items with n\\wikidata=.\n",
    "        if n.tags.get('train') == 'yes':\n",
    "            if n.tags.get('public_transport') == \"stop_position\":\n",
    "                self.stops[str(n.id)] = dict(n.tags)\n",
    "            if n.tags.get(\"wikidata\", \"\") != \"\":\n",
    "                self.stations[str(n.id)] = dict(n.tags)\n",
    "\n",
    "    def way(self, w):\n",
    "        # We filter on w\\railway=rail\n",
    "        if w.tags.get('railway') == 'rail':\n",
    "            self.track.append([ str(x) for x in w.nodes ])\n",
    "\n",
    "    def relation(self, r):\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s = StationHandler()\n",
    "s.apply_file(osm_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TrainData\n",
    "\n",
    "As the extracting from the full-sized dataset can take a long time (on my machine it took about 30 minutes for the NL dataset) we want to save the extracted data.  This is done by storing the data in a `TrainData` object that can be pickled."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainData:\n",
    "    stops: dict[str, dict[str, any]]\n",
    "    stations: dict[str, dict[str, any]]\n",
    "    track: list[list[str]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_data = TrainData(stops=s.stops, stations=s.stations, track=s.track)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "pickle_file = \"nl-train-data.pickle\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# with open(pickle_file, 'wb') as f:\n",
    "    # pickle.dump(train_data, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing\n",
    "\n",
    "Our goal is to create a graph where each vertex is a station and the edges represent train track between them.  Each station can have different stop positions.  For this reason we make use of the name of a stop position as they are the same for all stop positions at a station.\n",
    "\n",
    "To create our graph we need to follow different pieces of track (ways) from one stop position until we find a different stop position.  This process has been implemented in a recursive function with a limit of `10000` on the recursion depth."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "station_stops = set(train_data.stops.keys())\n",
    "stations_by_ele = { v['name']: (k,v) for k, v in train_data.stations.items() }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "graph = defaultdict(set)\n",
    "for track in train_data.track:\n",
    "    prev_node = None\n",
    "    for node in track:\n",
    "        if prev_node:\n",
    "            graph[prev_node].add(node)\n",
    "            graph[node].add(prev_node)\n",
    "        prev_node = node"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from functools import reduce\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "def get_station_at(loc):\n",
    "    stop = train_data.stops.get(loc)\n",
    "    if stop != None:\n",
    "        return stop.get('name', 'No name')\n",
    "    return None\n",
    "\n",
    "def find_next_station(station, loc, visited, depth):\n",
    "    \"\"\"Recursive function to find the next station (stop position) on the line.\n",
    "    \n",
    "    Is limited by the recursionlimit to avoid killing the kernel.\n",
    "    \"\"\"\n",
    "    if depth >= sys.getrecursionlimit():\n",
    "        return []\n",
    "    visited.add(loc)\n",
    "    st = get_station_at(loc)\n",
    "    if st != None and st != station:\n",
    "        return [ st ]\n",
    "\n",
    "    outgoing_track = [ x for x in graph[loc] if x not in visited ]\n",
    "    if len(outgoing_track) == 0:\n",
    "        return []\n",
    "\n",
    "    return reduce(lambda a,b:a+b, [ find_next_station(station, x, visited, depth + 1) for x in outgoing_track ], [])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "station_graph = defaultdict(set)\n",
    "for stop in train_data.stops.keys():\n",
    "    station = get_station_at(stop)\n",
    "    station_graph[station].update(find_next_station(station, stop, set(), 0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Station Graph\n",
    "\n",
    "Here we save the station graph to file such that we do not need to re-run the logic above a second time."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "# with open('graph-nl.json', 'w') as f:\n",
    "#     f.write(json.dumps({ name: list(values) for name, values in station_graph.items() }))\n",
    "\n",
    "with open('graph-nl.json', 'r') as f:\n",
    "    station_graph = defaultdict(set)\n",
    "    loaded_graph = json.loads(f.read())\n",
    "    for name, connections in loaded_graph.items():\n",
    "        if len(connections) > 0:\n",
    "            station_graph[name] = set(connections)\n",
    "\n",
    "print(f\"Found {len(station_graph)} stations!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation\n",
    "\n",
    "As a quick debugging tool we make use of `graphviz` to visualise the extracted graph."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import graphviz\n",
    "from collections import defaultdict\n",
    "\n",
    "def visualise(graph, name):\n",
    "    g = graphviz.Graph('G', filename=f\"{name}.gv\", engine='sfdp')\n",
    "\n",
    "    connected = defaultdict(set)\n",
    "    def is_connected(start, end):\n",
    "        return end in connected[start] or start in connected[end]\n",
    "\n",
    "    for station, linked_stations in graph.items():\n",
    "        for l in linked_stations:\n",
    "            if not is_connected(station, l) and station != l:\n",
    "                g.edge(station, l)\n",
    "                connected[station].add(l)\n",
    "\n",
    "    g.view()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Different Setups\n",
    "\n",
    "Now that we have a graph of stations and tracks between them we need to create different setups that represent more centralised setups.  To do this we define a function that correctly updates the graph while grouping a `nodeB` with `nodeA`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def group_nodes(graph, nodeA, nodeB):\n",
    "    \"\"\"Group two nodes A and B together into node A.\n",
    "\n",
    "    Delete node B, join all outgoing links from B with A, and update all \n",
    "    incoming links for B to A.\n",
    "    \"\"\"\n",
    "    outgoing_links = [ x for x in graph[nodeB] if x != nodeB ]\n",
    "    del graph[nodeB]\n",
    "    graph[nodeA] = (graph[nodeA] | set(outgoing_links)) - { nodeA, nodeB }\n",
    "    for node, values in graph.items():\n",
    "        if nodeB in values:\n",
    "            graph[nodeA].add(node)\n",
    "            graph[node] = (graph[node] - { nodeB }) | { nodeA }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the next block we define the main function that creates an alternate graph with a target in terms of number of nodes for the graph.  Over several iterations the function will decrease the graph by grouping the least occuring node with one of its neighbours.  The least occuring node is defined as the node that is still part of the graph (has not been grouped into a different node) and has the least amount of nodes pointing to it.  A node points to a different node when it is grouped into that different node.\n",
    "\n",
    "This strategy allows us to create a reasonably uniform distribution of mapped nodes.  This is important for the Hybrid strategies as a uniform distribution of nodes is important to get similar performance over the different groupings."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "def count_occurances(mapping) -> dict[str, int]:\n",
    "    \"\"\"Count the occurances of target nodes in the mapping.\"\"\"\n",
    "    counter = defaultdict(int)\n",
    "    for value in mapping.values():\n",
    "        counter[value] += 1\n",
    "    return counter\n",
    "\n",
    "def get_least_occuring_node(mapping: dict[str, str]) -> str:\n",
    "    \"\"\"Return a node that occurs the least in the mapping.\n",
    "\n",
    "    If there are multiple nodes that occur the least a random node is selected.\n",
    "    \"\"\"\n",
    "    occurances = count_occurances(mapping)\n",
    "    items = list(occurances.items())\n",
    "    random.shuffle(items)\n",
    "    return min(items, key=lambda x: x[1])[0]\n",
    "\n",
    "def create_mapping(mapping, from_node, to_node):\n",
    "    \"\"\"Create a mapping `from` to a different node.\n",
    "    \n",
    "    Updates the maping by inserting the new mapping and updating all existing \n",
    "    mappings that used to point to `from_node` to `to_node`.\n",
    "    \"\"\"\n",
    "    mapping[from_node] = to_node\n",
    "    for node, to in mapping.items():\n",
    "        if to == from_node:\n",
    "            mapping[node] = to_node\n",
    "\n",
    "def create_alternate_graph(original_graph, no_nodes: int):\n",
    "    alternate_graph = dict.copy(original_graph)\n",
    "    mapping = { node: node for node in alternate_graph.keys() }\n",
    "    while len(alternate_graph) > no_nodes:\n",
    "        main_node = get_least_occuring_node(mapping)\n",
    "        neighbouring_nodes = list(alternate_graph[main_node])\n",
    "        if len(neighbouring_nodes) == 0:\n",
    "            print(f\"No outgoing links: {main_node}\")\n",
    "            continue\n",
    "        neighbouring_node = random.choice(list(alternate_graph[main_node]))\n",
    "        group_nodes(alternate_graph, main_node, neighbouring_node)\n",
    "        create_mapping(mapping, neighbouring_node, main_node)\n",
    "    return alternate_graph, mapping"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The block below is used for quickly testing the grouping strategy and visualising the result."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "alternate_graph, mapping = create_alternate_graph(station_graph, 256)\n",
    "print(f\"Found {len(set(mapping.values()))} nodes in the mapping:\")\n",
    "print(json.dumps(count_occurances(mapping), indent=4))\n",
    "print(f\"Found {len(set(alternate_graph.keys()))} nodes in the graph:\")\n",
    "print(json.dumps({ node: list(values) for node, values in alternate_graph.items() }, indent=4))\n",
    "visualise(alternate_graph, \"test\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setups\n",
    "To test the effect of differently sized edge networks we want to create a subset of sizes that we will use."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "max_nodes = len(station_graph)\n",
    "setups = []\n",
    "no_nodes = 1\n",
    "while no_nodes < max_nodes:\n",
    "    setups.append(no_nodes)\n",
    "    no_nodes = no_nodes * 2\n",
    "setups.append(max_nodes)\n",
    "print(setups)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def clear_names(graph):\n",
    "    \"\"\"Sets more generic names (cdn1, cdn2, ..., cdnN) for the nodes in the graph.\"\"\"\n",
    "    name_map = { name: f\"cdn{n}\" for n, name in enumerate(graph.keys()) }\n",
    "    return { name_map[name]: set([ name_map[v] for v in values ]) for name, values in graph.items() }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "generic_graph = clear_names(station_graph)\n",
    "\n",
    "for size in setups:\n",
    "    alternate_graph, mapping = create_alternate_graph(generic_graph, size)\n",
    "    visualise(alternate_graph, str(size))\n",
    "    with open(f\"./out/setups/graph-{size}.json\", 'w') as f:\n",
    "        json.dump({ node: list(values) for node, values in alternate_graph.items()}, f)\n",
    "    with open(f\"./out/setups/mapping-{size}.json\", 'w') as f:\n",
    "        json.dump(mapping, f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "9db4aeea5ee0f72a1294a34462950b9f27f16fd151825e46b435a92327809acf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}